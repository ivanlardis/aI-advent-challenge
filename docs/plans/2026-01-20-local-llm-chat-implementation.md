# Local LLM Chat Application Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Chainlit Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ñ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸ÐµÐ¹ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾Ð¹ LLM (Ollama), ÑÐµÑÑÐ¸Ð¾Ð½Ð½Ð¾Ð¹ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÐµÐ¹ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð°, Ð²Ñ‹Ð±Ð¾Ñ€Ð¾Ð¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸.

**Architecture:** Chainlit Ð²ÐµÐ±-Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹Ñ Ñ Python backend, Ð¿Ñ€ÑÐ¼Ñ‹Ðµ HTTP Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ðº Ollama API Ñ‡ÐµÑ€ÐµÐ· Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÑƒ requests, ÑÐµÑÑÐ¸Ð¾Ð½Ð½Ð¾Ðµ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ð² Ð¿Ð°Ð¼ÑÑ‚Ð¸, stream Ð²Ñ‹Ð²Ð¾Ð´ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð².

**Tech Stack:** Python 3.10+, Chainlit 2.3.0, requests, Ollama API, Docker (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ Ð´Ð»Ñ Ollama)

---

## Task 1: Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¹

**Files:**
- Create: `app/__init__.py`
- Create: `app/ollama_client.py`
- Create: `app/history.py`
- Create: `app/chainlit_app.py`
- Create: `requirements.txt`
- Create: `.env.example`
- Create: `README.md`

**Step 1: Create app directory with __init__.py**

```bash
mkdir -p app
touch app/__init__.py
```

**Step 2: Verify structure**

Run: `ls -la app/`
Expected: `__init__.py` exists

**Step 3: Commit**

```bash
git add app/__init__.py
git commit -m "feat: create app directory structure"
```

---

## Task 2: Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ requirements.txt

**Files:**
- Create: `requirements.txt`

**Step 1: Write requirements.txt**

```bash
cat > requirements.txt << 'EOF'
chainlit==2.3.0
requests==2.32.3
python-dotenv==1.0.1
EOF
```

**Step 2: Verify file content**

Run: `cat requirements.txt`
Expected: Shows three dependencies with exact versions

**Step 3: Commit**

```bash
git add requirements.txt
git commit -m "deps: add chainlit, requests and python-dotenv"
```

---

## Task 3: Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ .env.example

**Files:**
- Create: `.env.example`

**Step 1: Write .env.example**

```bash
cat > .env.example << 'EOF'
# Ollama configuration
OLLAMA_HOST=http://localhost:11434
EOF
```

**Step 2: Verify file content**

Run: `cat .env.example`
Expected: Shows OLLAMA_HOST configuration example

**Step 3: Commit**

```bash
git add .env.example
git commit -m "config: add environment variables example"
```

---

## Task 4: Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ OllamaClient ÐºÐ»Ð°ÑÑ

**Files:**
- Create: `app/ollama_client.py`

**Step 1: Write OllamaClient implementation**

```python
import requests
import json
from typing import List, Dict, Iterator


class OllamaClient:
    """HTTP ÐºÐ»Ð¸ÐµÐ½Ñ‚ Ð´Ð»Ñ Ollama API."""

    def __init__(self, host: str = "http://localhost:11434"):
        self.host = host
        self.session = requests.Session()

    def list_models(self) -> List[Dict]:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÐ¿Ð¸ÑÐ¾Ðº Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹.

        Returns:
            Ð¡Ð¿Ð¸ÑÐ¾Ðº ÑÐ»Ð¾Ð²Ð°Ñ€ÐµÐ¹ Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÐµÐ¹ Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑÑ…

        Raises:
            requests.RequestException: ÐŸÑ€Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐµ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ
        """
        response = self.session.get(f"{self.host}/api/tags")
        response.raise_for_status()
        return response.json().get("models", [])

    def generate_stream(self, messages: List[Dict], model: str) -> Iterator[Dict]:
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚ Ð² streaming Ñ€ÐµÐ¶Ð¸Ð¼Ðµ.

        Args:
            messages: Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ Ollama API
            model: ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸

        Yields:
            Ð§Ð°Ð½ÐºÐ¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¾Ñ‚ Ollama API

        Raises:
            requests.RequestException: ÐŸÑ€Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐµ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸
        """
        payload = {
            "model": model,
            "messages": messages,
            "stream": True
        }

        response = self.session.post(
            f"{self.host}/api/chat",
            json=payload,
            stream=True
        )
        response.raise_for_status()

        for line in response.iter_lines():
            if line:
                yield json.loads(line)
```

**Step 2: Verify syntax**

Run: `python -m py_compile app/ollama_client.py`
Expected: No errors

**Step 3: Commit**

```bash
git add app/ollama_client.py
git commit -m "feat: add OllamaClient with list_models and generate_stream"
```

---

## Task 5: Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ SessionHistory ÐºÐ»Ð°ÑÑ

**Files:**
- Create: `app/history.py`

**Step 1: Write SessionHistory implementation**

```python
from datetime import datetime
from typing import List, Dict


class SessionHistory:
    """ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€ ÑÐµÑÑÐ¸Ð¾Ð½Ð½Ð¾Ð¹ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹."""

    def __init__(self):
        self.messages: List[Dict] = []

    def add_user_message(self, content: str) -> None:
        """Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÑ‚ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð² Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ.

        Args:
            content: Ð¢ÐµÐºÑÑ‚ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ
        """
        self.messages.append({
            "role": "user",
            "content": content,
            "timestamp": datetime.now().isoformat()
        })

    def add_assistant_message(self, content: str) -> None:
        """Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð° Ð² Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ.

        Args:
            content: Ð¢ÐµÐºÑÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð°
        """
        self.messages.append({
            "role": "assistant",
            "content": content,
            "timestamp": datetime.now().isoformat()
        })

    def get_for_api(self) -> List[Dict]:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ Ollama API.

        Returns:
            Ð¡Ð¿Ð¸ÑÐ¾Ðº ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ñ Ð¿Ð¾Ð»ÑÐ¼Ð¸ role Ð¸ content
        """
        return [
            {"role": m["role"], "content": m["content"]}
            for m in self.messages
        ]

    def clear(self) -> None:
        """ÐžÑ‡Ð¸Ñ‰Ð°ÐµÑ‚ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹."""
        self.messages.clear()
```

**Step 2: Verify syntax**

Run: `python -m py_compile app/history.py`
Expected: No errors

**Step 3: Commit**

```bash
git add app/history.py
git commit -m "feat: add SessionHistory for managing chat history"
```

---

## Task 6: Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Chainlit Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ

**Files:**
- Create: `app/chainlit_app.py`

**Step 1: Write chainlit_app.py implementation**

```python
import chainlit as cl
from app.ollama_client import OllamaClient
from app.history import SessionHistory
import time
import os

OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")


@cl.on_chat_start
async def on_chat_start():
    """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸Ðº Ð½Ð°Ñ‡Ð°Ð»Ð° Ñ‡Ð°Ñ‚Ð°: Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ðº Ollama Ð¸ Ð²Ñ‹Ð±Ð¾Ñ€ Ð¼Ð¾Ð´ÐµÐ»Ð¸."""
    try:
        client = OllamaClient(OLLAMA_HOST)
        models = client.list_models()

        if not models:
            await cl.Message(
                content=(
                    "âŒ ÐœÐ¾Ð´ÐµÐ»Ð¸ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹.\n\n"
                    "Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚Ðµ Ð¼Ð¾Ð´ÐµÐ»ÑŒ:\n"
                    "```bash\nollama pull gemma2:2b\n```\n\n"
                    "Ð˜Ð»Ð¸ Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ Ollama:\n"
                    "```bash\nollama serve\n```"
                )
            ).send()
            return

        # Dropdown Ð´Ð»Ñ Ð²Ñ‹Ð±Ð¾Ñ€Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸
        settings = await cl.ChatSettings(
            [
                cl.input_widget.Select(
                    id="model",
                    label="Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð¼Ð¾Ð´ÐµÐ»ÑŒ",
                    values=[m["name"] for m in models],
                    initial_index=0
                )
            ]
        ).send()

        model = settings["model"]
        history = SessionHistory()

        cl.user_session.set("model", model)
        cl.user_session.set("history", history)
        cl.user_session.set("client", client)

        await cl.Message(
            content=f"âœ… ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾ Ðº **{model}**\n\nÐžÑ‚Ð¿Ñ€Ð°Ð²ÑŒÑ‚Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ!"
        ).send()

    except requests.exceptions.ConnectionError:
        await cl.Message(
            content=(
                "âŒ Ollama Ð½Ðµ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½.\n\n"
                "Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ Ollama:\n"
                "```bash\nollama serve\n```"
            )
        ).send()
    except Exception as e:
        await cl.Message(content=f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: {e}").send()


@cl.on_message
async def on_message(message: cl.Message):
    """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸Ðº ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ: Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ñ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼Ð¸."""
    history = cl.user_session.get("history")
    client = cl.user_session.get("client")
    model = cl.user_session.get("model")

    history.add_user_message(message.content)

    start_time = time.time()
    response_content = ""

    msg = cl.Message(content="")
    await msg.send()

    try:
        for chunk in client.generate_stream(history.get_for_api(), model):
            if "message" in chunk:
                token = chunk["message"].get("content", "")
                response_content += token
                await msg.stream_token(token)

        duration = time.time() - start_time
        char_count = len(response_content)

        metrics = (
            f"\n\n---\n"
            f"â± {duration:.1f} ÑÐµÐº â€¢ ðŸ”¢ {char_count} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²"
        )
        await msg.stream_token(metrics)

        history.add_assistant_message(response_content)

    except Exception as e:
        await msg.stream_token(f"\n\nâŒ ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")


@cl.on_chat_end
async def on_chat_end():
    """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸Ðº Ð¾ÐºÐ¾Ð½Ñ‡Ð°Ð½Ð¸Ñ Ñ‡Ð°Ñ‚Ð°: Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ° ÑÐµÑÑÐ¸Ð¸."""
    history = cl.user_session.get("history")
    if history:
        history.clear()
```

**Step 2: Verify syntax**

Run: `python -m py_compile app/chainlit_app.py`
Expected: No errors

**Step 3: Commit**

```bash
git add app/chainlit_app.py
git commit -m "feat: add Chainlit app with chat handlers"
```

---

## Task 7: Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ README Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸ÑÐ¼Ð¸

**Files:**
- Create: `README.md`

**Step 1: Write README.md**

```markdown
# Local LLM Chat - Chainlit Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ

Chainlit Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ LLM Ñ‡ÐµÑ€ÐµÐ· Ollama.

## Ð’Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸

- ðŸ¤– Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸ Ñ‡ÐµÑ€ÐµÐ· Ollama
- ðŸ’¬ Ð¡ÐµÑÑÐ¸Ð¾Ð½Ð½Ð°Ñ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð°
- ðŸŽ› Ð’Ñ‹Ð±Ð¾Ñ€ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸Ð· ÑÐ¿Ð¸ÑÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ…
- ðŸ“Š ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ (Ð²Ñ€ÐµÐ¼Ñ, ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²)
- âš¡ Stream Ð²Ñ‹Ð²Ð¾Ð´ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð² Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸

## Ð¢Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ

- Python 3.10+
- Ollama (ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð¸ Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ)

## Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ°

### 1. Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ollama

**macOS:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 2. Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸

```bash
ollama pull gemma2:2b
```

Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸: https://ollama.com/library

### 3. Ð—Ð°Ð¿ÑƒÑÐº Ollama

```bash
ollama serve
```

### 4. Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÐµÐ¹ Python

```bash
pip install -r requirements.txt
```

## Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ

### 1. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)

Ð¡ÐºÐ¾Ð¿Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ `.env.example` Ð² `.env` Ð¸ Ð¸Ð·Ð¼ÐµÐ½Ð¸Ñ‚Ðµ Ñ…Ð¾ÑÑ‚ Ollama ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾:

```bash
cp .env.example .env
```

### 2. Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ

```bash
chainlit run app/chainlit_app.py
```

### 3. ÐžÑ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ Ð² Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ðµ

ÐžÑ‚ÐºÑ€Ð¾Ð¹Ñ‚Ðµ http://localhost:8000

## Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°

```
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chainlit_app.py    # Ð“Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð» Chainlit
â”‚   â”œâ”€â”€ ollama_client.py   # HTTP ÐºÐ»Ð¸ÐµÐ½Ñ‚ Ð´Ð»Ñ Ollama
â”‚   â””â”€â”€ history.py         # ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## API Ollama

ÐŸÑ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ollama API:
- `GET /api/tags` - ÑÐ¿Ð¸ÑÐ¾Ðº Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
- `POST /api/chat` - Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð°

Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ: https://github.com/ollama/ollama/blob/main/docs/api.md

## AI Advent Challenge 2025

Ð”ÐµÐ½ÑŒ 26: Ð’ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½ÑƒÑŽ LLM Ð² Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ.
```

**Step 2: Verify file content**

Run: `head -30 README.md`
Expected: Shows title and features

**Step 3: Commit**

```bash
git add README.md
git commit -m "docs: add README with installation and usage instructions"
```

---

## Task 8: Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ .gitignore

**Files:**
- Create: `.gitignore`

**Step 1: Write .gitignore**

```bash
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/
.venv

# Environment
.env

# Chainlit
.chainlit/

# IDE
.idea/
.vscode/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db
EOF
```

**Step 2: Verify file content**

Run: `cat .gitignore`
Expected: Shows Python, venv, .env, and IDE ignores

**Step 3: Commit**

```bash
git add .gitignore
git commit -m "chore: add gitignore for Python project"
```

---

## Task 9: Ð ÑƒÑ‡Ð½Ð¾Ðµ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ

**Files:**
- None (manual testing)

**Step 1: Install dependencies**

```bash
pip install -r requirements.txt
```

Expected: All packages install successfully

**Step 2: Ensure Ollama is running**

```bash
ollama list
```

Expected: Shows at least one installed model

If no models: `ollama pull gemma2:2b`

**Step 3: Start the application**

```bash
chainlit run app/chainlit_app.py -w
```

Expected: Server starts on http://localhost:8000

Flag `-w` enables auto-reload on code changes

**Step 4: Test in browser**

1. Open http://localhost:8000
2. Select model from dropdown
3. Send message: "ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð Ð°ÑÑÐºÐ°Ð¶Ð¸ Ð¾ ÑÐµÐ±Ðµ."
4. Verify:
   - Stream output appears in real-time
   - Metrics shown below response (â± time â€¢ ðŸ”¢ characters)
   - Send follow-up message to test history
   - Context is maintained in conversation

**Step 5: Test error handling**

Stop Ollama: `pkill ollama`

Refresh browser and verify error message appears

Start Ollama again: `ollama serve`

**Step 6: Commit final version**

```bash
git add .
git commit -m "test: manual testing complete - application working"
```

---

## Task 10: Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ .chainlit/config.toml Ð´Ð»Ñ ÐºÐ°ÑÑ‚Ð¾Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸

**Files:**
- Create: `.chainlit/config.toml`

**Step 1: Create .chainlit directory and config**

```bash
mkdir -p .chainlit
cat > .chainlit/config.toml << 'EOF'
[UI]
# ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ
name = "Local LLM Chat"

# ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ
description = "Ð§Ð°Ñ‚ Ñ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ LLM Ñ‡ÐµÑ€ÐµÐ· Ollama"

# Ð˜ÐºÐ¾Ð½ÐºÐ° (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ Chainlit)
# default_collapse_message = false

[project]
# Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ðµ
# theme = "default"
EOF
```

**Step 2: Verify directory structure**

Run: `ls -la .chainlit/`
Expected: `config.toml` exists

**Step 3: Commit**

```bash
git add .chainlit/config.toml
git commit -m "feat: add Chainlit configuration"
```

---

## Ð—Ð°Ð´Ð°Ð½Ð¸Ñ Ð´Ð»Ñ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ (ÐµÑÐ»Ð¸ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ)

Ð•ÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ‚ÐµÑÑ‚Ñ‹:

1. **Ð¢ÐµÑÑ‚ OllamaClient**: Ð¼Ð¾ÐºÐ¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ requests.Session Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÑ‚ÑŒ Ð²Ñ‹Ð·Ð¾Ð²Ñ‹ API
2. **Ð¢ÐµÑÑ‚ SessionHistory**: Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÑ‚ÑŒ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð¸ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ Ð´Ð»Ñ API
3. **Ð˜Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ðµ Ñ‚ÐµÑÑ‚Ñ‹**: Ð·Ð°Ð¿ÑƒÑÐºÐ°Ñ‚ÑŒ Chainlit Ñ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¼ Ollama server

Ð”Ð»Ñ Day 26 Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ñ€ÑƒÑ‡Ð½Ð¾Ð³Ð¾ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ.

---

## Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ°

**Step 1: Verify all files exist**

```bash
find . -type f -name "*.py" -o -name "*.txt" -o -name "*.md" -o -name ".env.example" | grep -v ".git" | sort
```

Expected:
```
.app/__init__.py
.app/chainlit_app.py
.app/history.py
.app/ollama_client.py
.chainlit/config.toml
.env.example
.gitignore
README.md
requirements.txt
```

**Step 2: Verify git log**

```bash
git log --oneline -10
```

Expected: Series of commits with proper messages

**Step 3: Final commit if needed**

```bash
git add .
git commit -m "feat: complete local LLM chat application for Day 26"
```

---

## Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÑ‚Ð¾Ð³Ð¾ Ð¿Ð»Ð°Ð½Ð°

Ð­Ñ‚Ð¾Ñ‚ Ð¿Ð»Ð°Ð½ Ð³Ð¾Ñ‚Ð¾Ð² Ðº Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ ÑÑƒÐ¿ÐµÑ€Ð½Ð°Ð²Ñ‹Ðº `superpowers:executing-plans` Ð´Ð»Ñ Ð¿Ð¾ÑˆÐ°Ð³Ð¾Ð²Ð¾Ð³Ð¾ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ.

ÐšÐ°Ð¶Ð´Ð°Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° Ñ€Ð°Ð·Ð±Ð¸Ñ‚Ð° Ð½Ð° Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ðµ ÑˆÐ°Ð³Ð¸:
1. ÐÐ°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ ÐºÐ¾Ð´/Ñ‚ÐµÑÑ‚
2. ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ (ÐºÐ¾Ð¼Ð°Ð½Ð´Ð° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸)
3. Ð—Ð°ÐºÐ¾Ð¼Ð¼Ð¸Ñ‚Ð¸Ñ‚ÑŒ

DRY, YAGNI, TDD, Ñ‡Ð°ÑÑ‚Ñ‹Ðµ ÐºÐ¾Ð¼Ð¼Ð¸Ñ‚Ñ‹.
