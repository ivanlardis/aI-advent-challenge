import chainlit as cl
from app.ollama_client import OllamaClient
import time
import os

OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")


@cl.on_message
async def on_message(message: cl.Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞."""
    # –°–æ–∑–¥–∞—ë–º –∫–ª–∏–µ–Ω—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
    client = OllamaClient(OLLAMA_HOST)

    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –∏ –≤—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—É—é
    models = client.list_models()
    if not models:
        await cl.Message(content="‚ùå –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã").send()
        return

    model = models[0]["name"]

    start_time = time.time()
    response_content = ""

    msg = cl.Message(content="")
    await msg.send()

    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –±–µ–∑ –∏—Å—Ç–æ—Ä–∏–∏ (–∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è)
        messages = [{"role": "user", "content": message.content}]

        for chunk in client.generate_stream(messages, model):
            if "message" in chunk:
                token = chunk["message"].get("content", "")
                response_content += token
                await msg.stream_token(token)

        duration = time.time() - start_time
        char_count = len(response_content)

        metrics = (
            f"\n\n---\n"
            f"‚è± {duration:.1f} —Å–µ–∫ ‚Ä¢ üî¢ {char_count} —Å–∏–º–≤–æ–ª–æ–≤"
        )
        await msg.stream_token(metrics)

    except Exception as e:
        await msg.stream_token(f"\n\n‚ùå –û—à–∏–±–∫–∞: {e}")
