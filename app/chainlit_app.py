import chainlit as cl
from app.ollama_client import OllamaClient
import time
import os
import httpx
import asyncio
from typing import Optional, List
from dataclasses import dataclass

OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
REQUEST_TIMEOUT = 10.0  # –¢–∞–π–º–∞—É—Ç –Ω–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å

# === –£—Å–ª–æ–∂–Ω—ë–Ω–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç ===
TEST_DATA = [
    # POSITIVE - —è–≤–Ω—ã–µ (5)
    ("–û—Ç–ª–∏—á–Ω—ã–π —Ç–æ–≤–∞—Ä, –æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω –ø–æ–∫—É–ø–∫–æ–π!", "positive"),
    ("–†–µ–∫–æ–º–µ–Ω–¥—É—é –≤—Å–µ–º, –ª—É—á—à–µ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ü–µ–Ω–∞-–∫–∞—á–µ—Å—Ç–≤–æ", "positive"),
    ("–ü—Ä–µ–≤–∑–æ—à—ë–ª –≤—Å–µ –º–æ–∏ –æ–∂–∏–¥–∞–Ω–∏—è!", "positive"),
    ("–ö–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –≤—ã—Å–æ—Ç–µ, –±—É–¥—É –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â—ë", "positive"),
    ("–ü—Ä–µ–∫—Ä–∞—Å–Ω—ã–π —Å–µ—Ä–≤–∏—Å –∏ –æ—Ç–ª–∏—á–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç", "positive"),

    # POSITIVE - —Å–ª–æ–∂–Ω—ã–µ (3)
    ("–°–Ω–∞—á–∞–ª–∞ —Å–æ–º–Ω–µ–≤–∞–ª—Å—è, –Ω–æ –≤ –∏—Ç–æ–≥–µ –Ω–µ –ø–æ–∂–∞–ª–µ–ª –æ –ø–æ–∫—É–ø–∫–µ", "positive"),
    ("–ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –≤—ã—Å–æ–∫—É—é —Ü–µ–Ω—É, —Ç–æ–≤–∞—Ä —Ç–æ–≥–æ —Å—Ç–æ–∏—Ç", "positive"),
    ("–î–æ–ª–≥–æ –≤—ã–±–∏—Ä–∞–ª –º–µ–∂–¥—É –∞–Ω–∞–ª–æ–≥–∞–º–∏, –∏ —ç—Ç–æ—Ç –æ–∫–∞–∑–∞–ª—Å—è –ª—É—á—à–∏–º", "positive"),

    # NEGATIVE - —è–≤–Ω—ã–µ (5)
    ("–£–∂–∞—Å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –¥–µ–Ω—å–≥–∏ –Ω–∞ –≤–µ—Ç–µ—Ä", "negative"),
    ("–ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é, –ø–æ–ª–Ω–æ–µ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ", "negative"),
    ("–°–ª–æ–º–∞–ª—Å—è —á–µ—Ä–µ–∑ –¥–≤–∞ –¥–Ω—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è", "negative"),
    ("–•—É–¥—à–∞—è –ø–æ–∫—É–ø–∫–∞ –≤ –º–æ–µ–π –∂–∏–∑–Ω–∏", "negative"),
    ("–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç, –ø—Ä–æ–±–ª–µ–º—É –Ω–µ —Ä–µ—à–∏–ª–∏", "negative"),

    # NEGATIVE - —Å–ª–æ–∂–Ω—ã–µ/—Å–∞—Ä–∫–∞–∑–º (3)
    ("–ù—É –¥–∞, –∫–æ–Ω–µ—á–Ω–æ, —Å—É–ø–µ—Ä –∫–∞—á–µ—Å—Ç–≤–æ... –µ—Å–ª–∏ –≤–∞–º –Ω—Ä–∞–≤–∏—Ç—Å—è –º—É—Å–æ—Ä", "negative"),
    ("–°–ø–∞—Å–∏–±–æ –∑–∞ –ø–æ—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è –∏ –Ω–µ—Ä–≤—ã", "negative"),
    ("–û—Ç–ª–∏—á–Ω—ã–π —Å–ø–æ—Å–æ–± –≤—ã–±—Ä–æ—Å–∏—Ç—å –¥–µ–Ω—å–≥–∏", "negative"),

    # NEUTRAL - —è–≤–Ω—ã–µ (4)
    ("–¢–æ–≤–∞—Ä —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—é", "neutral"),
    ("–û–±—ã—á–Ω—ã–π —Ç–æ–≤–∞—Ä, –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ", "neutral"),
    ("–î–æ—Å—Ç–∞–≤–∏–ª–∏ –≤–æ–≤—Ä–µ–º—è", "neutral"),
    ("–ù–æ—Ä–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞ —Å–≤–æ—é —Ü–µ–Ω—É", "neutral"),

    # NEUTRAL - —Å–ª–æ–∂–Ω—ã–µ/—Å–º–µ—à–∞–Ω–Ω—ã–µ (4)
    ("–ï—Å—Ç—å –ø–ª—é—Å—ã –∏ –º–∏–Ω—É—Å—ã, –≤ —Ü–µ–ª–æ–º –Ω–æ—Ä–º–∞–ª—å–Ω–æ", "neutral"),
    ("–ö–∞—á–µ—Å—Ç–≤–æ —Ö–æ—Ä–æ—à–µ–µ, –Ω–æ —Ü–µ–Ω–∞ –∑–∞–≤—ã—à–µ–Ω–∞", "neutral"),
    ("–†–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –∑–∞—è–≤–ª–µ–Ω–æ, –Ω–µ –±–æ–ª—å—à–µ –∏ –Ω–µ –º–µ–Ω—å—à–µ", "neutral"),
    ("–û–∂–∏–¥–∞–ª –±–æ–ª—å—à–µ–≥–æ, –Ω–æ –∏ –Ω–µ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω", "neutral"),
]

# === –í–∞—Ä–∏–∞–Ω—Ç—ã –ø—Ä–æ–º–ø—Ç–æ–≤ ===
PROMPTS = {
    "baseline": None,
    "zero-shot": """–¢—ã ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞.
–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: positive, negative –∏–ª–∏ neutral.
–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: positive, negative –∏–ª–∏ neutral.""",
    "few-shot": """–¢—ã ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–∑—ã–≤–æ–≤.
–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: positive, negative –∏–ª–∏ neutral.

–ü—Ä–∏–º–µ—Ä—ã:
- "–û—Ç–ª–∏—á–Ω—ã–π —Ç–æ–≤–∞—Ä!" ‚Üí positive
- "–°–Ω–∞—á–∞–ª–∞ —Å–æ–º–Ω–µ–≤–∞–ª—Å—è, –Ω–æ –Ω–µ –ø–æ–∂–∞–ª–µ–ª" ‚Üí positive
- "–£–∂–∞—Å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ" ‚Üí negative
- "–ù—É –¥–∞, —Å—É–ø–µ—Ä... –µ—Å–ª–∏ –ª—é–±–∏—Ç–µ –º—É—Å–æ—Ä" ‚Üí negative (—Å–∞—Ä–∫–∞–∑–º)
- "–¢–æ–≤–∞—Ä –∫–∞–∫ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏" ‚Üí neutral
- "–ï—Å—Ç—å –ø–ª—é—Å—ã –∏ –º–∏–Ω—É—Å—ã" ‚Üí neutral

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: positive, negative –∏–ª–∏ neutral.""",
}


@dataclass
class ExperimentResult:
    model: str
    prompt_type: str
    correct: int
    total: int
    avg_time: float

    @property
    def accuracy(self) -> float:
        return self.correct / self.total * 100 if self.total > 0 else 0


async def get_available_models(host: str) -> List[str]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (async)."""
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(f"{host}/api/tags")
            response.raise_for_status()
            return [m["name"] for m in response.json().get("models", [])]
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
        return []


async def generate_async(
    host: str,
    prompt: str,
    model: str,
    system: Optional[str] = None,
    temperature: Optional[float] = None,
    num_ctx: Optional[int] = None,
) -> tuple[str, float]:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞."""
    messages = []
    if system:
        messages.append({"role": "system", "content": system})
    messages.append({"role": "user", "content": prompt})

    payload = {
        "model": model,
        "messages": messages,
        "stream": False,
    }

    options = {}
    if temperature is not None:
        options["temperature"] = temperature
    if num_ctx is not None:
        options["num_ctx"] = num_ctx
    if options:
        payload["options"] = options

    start_time = time.time()

    async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT) as client:
        response = await client.post(f"{host}/api/chat", json=payload)
        response.raise_for_status()

    elapsed = time.time() - start_time
    data = response.json()
    content = data.get("message", {}).get("content", "")
    return content.strip(), elapsed


def parse_sentiment(response: str) -> Optional[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç sentiment –∏–∑ –æ—Ç–≤–µ—Ç–∞."""
    response_lower = response.lower().strip()
    if response_lower in ("positive", "negative", "neutral"):
        return response_lower
    for sentiment in ("positive", "negative", "neutral"):
        if sentiment in response_lower:
            return sentiment
    return None


async def run_experiment(
    msg: cl.Message,
    model: str,
    prompt_type: str,
    system_prompt: Optional[str],
) -> ExperimentResult:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–∏–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–º–æ–¥–µ–ª—å + –ø—Ä–æ–º–ø—Ç)."""
    correct = 0
    times = []

    temp = 0.0 if prompt_type != "baseline" else None
    ctx = 2048 if prompt_type != "baseline" else None

    for i, (text, expected) in enumerate(TEST_DATA, 1):
        try:
            response, elapsed = await generate_async(
                host=OLLAMA_HOST,
                prompt=text,
                model=model,
                system=system_prompt,
                temperature=temp,
                num_ctx=ctx,
            )
            times.append(elapsed)
            predicted = parse_sentiment(response)

            if predicted is None:
                status = "‚ùì"
            elif predicted == expected:
                correct += 1
                status = "‚úÖ"
            else:
                status = "‚ùå"

            short_text = text[:25] + "..." if len(text) > 25 else text
            await msg.stream_token(f"`[{i:2}]` {status} {expected:8} ‚Üí {response[:12]:12} | {short_text}\n")

        except httpx.TimeoutException:
            await msg.stream_token(f"`[{i:2}]` ‚è±Ô∏è –¢–∞–π–º–∞—É—Ç!\n")
        except Exception as e:
            await msg.stream_token(f"`[{i:2}]` ‚ö†Ô∏è {str(e)[:30]}\n")

    avg_time = sum(times) / len(times) if times else 0

    return ExperimentResult(
        model=model,
        prompt_type=prompt_type,
        correct=correct,
        total=len(TEST_DATA),
        avg_time=avg_time,
    )


async def run_benchmark(msg: cl.Message):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫: –≤—Å–µ –º–æ–¥–µ–ª–∏ √ó –≤—Å–µ –ø—Ä–æ–º–ø—Ç—ã."""

    models = await get_available_models(OLLAMA_HOST)
    if not models:
        await msg.stream_token("‚ùå –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!\n")
        return

    total_experiments = len(models) * len(PROMPTS)

    await msg.stream_token("# üß™ –ë–µ–Ω—á–º–∞—Ä–∫: –ú–æ–¥–µ–ª–∏ √ó –ü—Ä–æ–º–ø—Ç—ã\n\n")
    await msg.stream_token(f"**–ú–æ–¥–µ–ª–∏:** {', '.join(models)}\n")
    await msg.stream_token(f"**–ü—Ä–æ–º–ø—Ç—ã:** {', '.join(PROMPTS.keys())}\n")
    await msg.stream_token(f"**–¢–µ—Å—Ç–æ–≤:** {len(TEST_DATA)} √ó {total_experiments} = {len(TEST_DATA) * total_experiments}\n\n")

    results: List[ExperimentResult] = []
    exp_num = 0

    for model in models:
        await msg.stream_token(f"---\n# üì¶ –ú–æ–¥–µ–ª—å: `{model}`\n\n")

        # –ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏ (–ø–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç—å)
        await msg.stream_token("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...\n")
        try:
            await generate_async(OLLAMA_HOST, "Hi", model, temperature=0.0, num_ctx=512)
            await msg.stream_token("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n\n")
        except Exception as e:
            await msg.stream_token(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\n\n")

        for prompt_type, system_prompt in PROMPTS.items():
            exp_num += 1
            await msg.stream_token(f"## [{exp_num}/{total_experiments}] {prompt_type}\n\n")

            result = await run_experiment(msg, model, prompt_type, system_prompt)
            results.append(result)

            await msg.stream_token(f"\n**Accuracy: {result.accuracy:.0f}%** ({result.correct}/{result.total})")
            await msg.stream_token(f" ‚Ä¢ –í—Ä–µ–º—è: {result.avg_time:.2f}s\n\n")

    # === –°–í–û–î–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê ===
    await msg.stream_token("---\n# üìä –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞\n\n")

    header = "| –ú–æ–¥–µ–ª—å |"
    separator = "|--------|"
    for pt in PROMPTS.keys():
        header += f" {pt} |"
        separator += "--------|"
    await msg.stream_token(header + "\n")
    await msg.stream_token(separator + "\n")

    for model in models:
        row = f"| {model} |"
        for prompt_type in PROMPTS.keys():
            r = next((x for x in results if x.model == model and x.prompt_type == prompt_type), None)
            if r:
                row += f" {r.accuracy:.0f}% |"
            else:
                row += " - |"
        await msg.stream_token(row + "\n")

    # === –õ–£–ß–®–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ ===
    await msg.stream_token("\n---\n# üèÜ –õ—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n\n")

    best = max(results, key=lambda r: r.accuracy)
    await msg.stream_token(f"**ü•á –õ—É—á—à–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è:** `{best.model}` + `{best.prompt_type}` = **{best.accuracy:.0f}%**\n\n")

    model_avg = {}
    for model in models:
        model_results = [r for r in results if r.model == model]
        model_avg[model] = sum(r.accuracy for r in model_results) / len(model_results)

    best_model = max(model_avg, key=model_avg.get)
    await msg.stream_token(f"**üì¶ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å (avg):** `{best_model}` = **{model_avg[best_model]:.0f}%**\n\n")

    prompt_avg = {}
    for pt in PROMPTS.keys():
        pt_results = [r for r in results if r.prompt_type == pt]
        prompt_avg[pt] = sum(r.accuracy for r in pt_results) / len(pt_results)

    best_prompt = max(prompt_avg, key=prompt_avg.get)
    await msg.stream_token(f"**üìù –õ—É—á—à–∏–π –ø—Ä–æ–º–ø—Ç (avg):** `{best_prompt}` = **{prompt_avg[best_prompt]:.0f}%**\n\n")

    # –£–ª—É—á—à–µ–Ω–∏–µ –æ—Ç baseline –∫ few-shot
    await msg.stream_token("---\n# üìà –£–ª—É—á—à–µ–Ω–∏–µ –æ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n\n")
    await msg.stream_token("| –ú–æ–¥–µ–ª—å | baseline ‚Üí few-shot | –ü—Ä–∏—Ä–æ—Å—Ç |\n")
    await msg.stream_token("|--------|---------------------|--------|\n")

    for model in models:
        baseline = next((r for r in results if r.model == model and r.prompt_type == "baseline"), None)
        fewshot = next((r for r in results if r.model == model and r.prompt_type == "few-shot"), None)
        if baseline and fewshot:
            improvement = fewshot.accuracy - baseline.accuracy
            await msg.stream_token(f"| {model} | {baseline.accuracy:.0f}% ‚Üí {fewshot.accuracy:.0f}% | **+{improvement:.0f}%** |\n")


@cl.on_message
async def on_message(message: cl.Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    client = OllamaClient(OLLAMA_HOST)

    if message.content.strip().lower() == "/benchmark":
        msg = cl.Message(content="")
        await msg.send()
        await run_benchmark(msg)
        return

    models = client.list_models()
    if not models:
        await cl.Message(content="‚ùå –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã").send()
        return

    model = models[0]["name"]
    start_time = time.time()
    response_content = ""

    msg = cl.Message(content="")
    await msg.send()

    try:
        messages = [{"role": "user", "content": message.content}]

        for chunk in client.generate_stream(messages, model):
            if "message" in chunk:
                token = chunk["message"].get("content", "")
                response_content += token
                await msg.stream_token(token)

        duration = time.time() - start_time
        char_count = len(response_content)

        metrics = f"\n\n---\n‚è± {duration:.1f} —Å–µ–∫ ‚Ä¢ üî¢ {char_count} —Å–∏–º–≤–æ–ª–æ–≤"
        await msg.stream_token(metrics)

    except Exception as e:
        await msg.stream_token(f"\n\n‚ùå –û—à–∏–±–∫–∞: {e}")
