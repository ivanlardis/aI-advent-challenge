import chainlit as cl
from app.ollama_client import OllamaClient
from app.history import SessionHistory
import time
import os
import requests

OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")


@cl.on_chat_start
async def on_chat_start():
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–∞—á–∞–ª–∞ —á–∞—Ç–∞: –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama –∏ –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏."""
    try:
        client = OllamaClient(OLLAMA_HOST)
        models = client.list_models()

        if not models:
            await cl.Message(
                content=(
                    "‚ùå –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.\n\n"
                    "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –º–æ–¥–µ–ª—å:\n"
                    "```bash\nollama pull gemma2:2b\n```\n\n"
                    "–ò–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ Ollama:\n"
                    "```bash\nollama serve\n```"
                )
            ).send()
            return

        # Dropdown –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
        settings = await cl.ChatSettings(
            [
                cl.input_widget.Select(
                    id="model",
                    label="–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å",
                    values=[m["name"] for m in models],
                    initial_index=0
                )
            ]
        ).send()

        model = settings["model"]
        history = SessionHistory()

        cl.user_session.set("model", model)
        cl.user_session.set("history", history)
        cl.user_session.set("client", client)

        await cl.Message(
            content=f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ **{model}**\n\n–û—Ç–ø—Ä–∞–≤—å—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ!"
        ).send()

    except requests.exceptions.ConnectionError:
        await cl.Message(
            content=(
                "‚ùå Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω.\n\n"
                "–ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama:\n"
                "```bash\nollama serve\n```"
            )
        ).send()
    except Exception as e:
        await cl.Message(content=f"‚ùå –û—à–∏–±–∫–∞: {e}").send()


@cl.on_message
async def on_message(message: cl.Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏."""
    history = cl.user_session.get("history")
    client = cl.user_session.get("client")
    model = cl.user_session.get("model")

    history.add_user_message(message.content)

    start_time = time.time()
    response_content = ""

    msg = cl.Message(content="")
    await msg.send()

    try:
        for chunk in client.generate_stream(history.get_for_api(), model):
            if "message" in chunk:
                token = chunk["message"].get("content", "")
                response_content += token
                await msg.stream_token(token)

        duration = time.time() - start_time
        char_count = len(response_content)

        metrics = (
            f"\n\n---\n"
            f"‚è± {duration:.1f} —Å–µ–∫ ‚Ä¢ üî¢ {char_count} —Å–∏–º–≤–æ–ª–æ–≤"
        )
        await msg.stream_token(metrics)

        history.add_assistant_message(response_content)

    except Exception as e:
        await msg.stream_token(f"\n\n‚ùå –û—à–∏–±–∫–∞: {e}")


@cl.on_chat_end
async def on_chat_end():
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ–∫–æ–Ω—á–∞–Ω–∏—è —á–∞—Ç–∞: –æ—á–∏—Å—Ç–∫–∞ —Å–µ—Å—Å–∏–∏."""
    history = cl.user_session.get("history")
    if history:
        history.clear()
