# AI Advent Challenge

## Задание 1
- Формулировка: реализовать простого агента, который отвечает на вопросы и выводит это в интерфейсе (простой чат, отправка/получение запросов через HTTP-клиент).
- Реализация: Chainlit UI + LangChain цепочка, проксирующая запросы в OpenRouter.
- Результат: агент принимает запрос и корректно вызывает инструмент (OpenRouter), ответ отображается в чате.
- Формат сдачи: код + демонстрационное видео (запуск `chainlit run ...` и работа через http://localhost:8000).
- Рекомендуемый префикс коммита: `chore: AI Advent Challenge 1 – ...`.

## Задание 2
- Формулировка: научиться задавать формат результата для возвращения от LLM, продемонстрировать умение парсить структурированные ответы.
- Реализация: использование `JsonOutputParser` из LangChain для получения структурированного JSON-ответа. Пример использования: анализ настроения текста.
- Результат: LLM возвращает ответ в формате JSON с полями `sentiment`, `confidence`, `keywords`, `summary`. JSON автоматически парсится и валидируется, результат отображается в красивом формате с сырым JSON.
- Технические детали:
  - `JsonOutputParser` автоматически добавляет `{format_instructions}` в промпт
  - Автоматическая валидация и парсинг JSON
  - Обработка ошибок при невалидном формате
- Формат сдачи: код + демонстрация работы (отправка текста и получение структурированного анализа).
- Рекомендуемый префикс коммита: `feat: AI Advent Challenge 2 – ...`.

## Задание 3
- Формулировка: реализовать конверсационного агента, который собирает информацию через диалог и сам определяет момент завершения.
- Реализация: бот-нутрициолог для расчёта БЖУ с автоматическим завершением диалога.
- Результат:
  - Модель задаёт 5 вопросов (рост, вес, возраст, пол, уровень активности)
  - Собирает ответы через историю диалога
  - Автоматически завершает диалог через флаг `is_complete=true`
  - Рассчитывает BMR, калорийность и БЖУ (30% белки, 30% жиры, 40% углеводы)
  - Возвращает расчёт в формате Markdown
- Технические детали:
  - JSON-структура: `{is_complete, message, collected_info, final_document}`
  - История сообщений: простой список с `HumanMessage` и `AIMessage`
  - JsonOutputParser для валидации и парсинга ответов
  - Отключены размышления модели (`reasoning_effort: disabled`)
  - Рекомендуемая модель: `x-ai/grok-4.1-fast:free`
- Формат сдачи: код + демонстрационное видео.
- Префикс коммита: `feat: AI Advent Challenge 3 – ...`

## Задание 4
- Формулировка: исследовать влияние system prompt на поведение агента, сохраняя историю диалога при смене промпта.
- Реализация: интерфейс для смены system prompt с 4 предустановленными ролями + сохранение истории сообщений.
- Результат:
  - UI для выбора роли агента (⚙️ в верхнем правом углу Chainlit)
  - 4 предустановленные роли:
    - **nutritionist** — нутрициолог для расчёта БЖУ (строгий формат, JSON-ответы)
    - **strict_teacher** — строгий преподаватель Python (краткие ответы, наводящие вопросы)
    - **friendly_mentor** — дружелюбный наставник (простые объяснения, много примеров)
    - **code_reviewer** — критичный код-ревьюер (детальный анализ, указание проблем)
  - История сообщений сохраняется при смене роли
  - System prompt применяется динамически из user session
- План тестирования:
  1. Выбрать тему (например: "объяснение рекурсии в Python")
  2. Начать диалог с `strict_teacher` (5-10 сообщений)
  3. Зафиксировать стиль ответов (тон, структура, глубина)
  4. Переключиться на `friendly_mentor`
  5. Продолжить диалог на ту же тему
  6. Сравнить различия в стиле, подаче информации, вопросах
  7. Повторить с другими ролями для разных сценариев
- Технические детали:
  - `cl.ChatSettings` с `cl.input_widget.Select` для UI
  - `@cl.on_settings_update` для обработки смены роли
  - Динамическая подстановка system prompt через `cl.user_session.get()`
  - История сохраняется в сессии независимо от system prompt
- Формат сдачи: код + демонстрация (видео с переключением ролей и сравнением ответов).
- Префикс коммита: `feat: AI Advent Challenge 4 – ...`

## Задание 5
- Формулировка: исследовать влияние параметра `temperature` на генерацию ответов LLM.
- Реализация: команда `/experiment <промпт>` для запуска любого промпта с тремя температурами (0.0, 0.3, 1.0).
- Результат:
  - Пользователь указывает свой промпт в команде
  - Последовательный запуск с тремя разными температурами
  - Визуальное отображение результатов в UI с эмодзи и описаниями
- Технические детали:
  - Динамический параметр `temperature` в `OpenRouterClient.chat_completion()`
  - Обработка команды `/experiment <текст>` в `@cl.on_message`
  - Парсинг пользовательского промпта из команды
  - Форматирование результатов через Markdown в `cl.Message`
- Примеры использования:
  - `/experiment Объясни что такое рекурсия`
  - `/experiment Напиши короткую историю про программиста`
  - `/experiment Опиши алгоритм быстрой сортировки`
- Выводы о temperature:
  - **0.0** — максимально детерминированно: почти всегда выбирается самый вероятный токен (минимум творчества, максимум повторяемости)
  - **0.3** — аккуратные, "сдержанные" ответы, меньше вариативности
  - **1.0** — максимум разнообразия в рамках нормального режима; больше риск "галлюцинаций"/нестабильности формулировок
- Формат сдачи: код + демонстрация (видео с запуском `/experiment` с разными промптами).
- Префикс коммита: `feat: AI Advent Challenge 5 – temperature experiments`

## Задание 6
- Формулировка: расширить эксперименты с температурой, тестируя больше значений для большей детализации.
- Реализация: команда `/experiment` обновлена для запуска с 4 температурами: 0.1, 1.0, 1.5, 1.9.
- Результат:
  - Параллельный запуск всех тестов для ускорения
  - Визуальное отображение с заголовками температур
  - Расширенный диапазон для выявления экстремальных режимов
- Технические детали:
  - Список температур: `[0.1, 1.0, 1.5, 1.9]`
  - System prompt для краткости: "Отвечай кратко, 5 предложений. Не размышляй ответь быстро не думая"
  - Форматирование через Markdown с разделителями
- Выводы:
  - **0.1** — практически детерминированно, очень предсказуемо
  - **1.0** — сбалансированное разнообразие
  - **1.5** — заметно больше креативности и неожиданных формулировок
  - **1.9** — экстремальная вариативность, возможны нелогичные ответы
- Формат сдачи: код + демонстрация (видео с запуском `/experiment`).
- Префикс коммита: `feat: AI Advent Challenge 6 – temperature experiments`

## Задание 7
- Формулировка: сравнить несколько версий моделей по скорости, качеству ответов и стоимости.
- Реализация: команда `/compare <промпт>` для параллельного тестирования 4 моделей OpenRouter.
- Результат:
  - Параллельное выполнение запросов через `asyncio.gather()`
  - Замер времени выполнения для каждой модели
  - Извлечение метрик из API: prompt_tokens, completion_tokens, total_tokens
  - Определение стоимости из API (поле `data.cost`) или пометка "FREE"
  - Красивая таблица с метриками производительности
  - Детальные ответы каждой модели для качественного сравнения
  - Автоматический анализ (самая быстрая, самая экономная)
- Технические детали:
  - Новый метод `OpenRouterClient.compare_models()` для параллельных запросов
  - Параметр `model` добавлен в метод `chat_completion()`
  - Обработка ошибок: HTTP 429 (rate limit), 404 (model not found), timeout
  - Функция `format_comparison_results()` для форматирования в Markdown
  - Интеграция команды `/compare` через проверку в `@cl.on_message`
- Модели для сравнения:
  - `nvidia/nemotron-nano-12b-v2-vl:free` — бесплатная модель Nvidia (12B параметров)
  - `openai/chatgpt-4o-latest` — последний ChatGPT ($5/$15 за 1M токенов prompt/completion)
  - `qwen/qwen-2.5-72b-instruct` — средняя китайская модель Qwen 2.5 ($0.07/$0.26 за 1M токенов)
  - `tngtech/deepseek-r1t2-chimera:free` — текущая baseline модель (бесплатная)
- Расчёт стоимости:
  - Прайс-лист встроен в код на основе данных из OpenRouter API
  - Формула: `(prompt_tokens / 1_000_000) * prompt_price + (completion_tokens / 1_000_000) * completion_price`
  - Бесплатные модели отображаются как "FREE"
  - Платные модели показывают реальную стоимость запроса в USD
- Примеры использования:
  - `/compare Объясни ООП в 3 предложениях`
  - `/compare Напиши функцию быстрой сортировки на Python`
  - `/compare Что такое замыкание в JavaScript?`
- Выводы:
  - Разные модели показывают различную скорость (от 2 до 10+ секунд)
  - ChatGPT обычно даёт самые качественные ответы, но платная
  - Llama 3.2 3B — очень быстрая, но менее детальная
  - Qwen 2.5 72B — хороший баланс качества и скорости
  - Количество токенов зависит от стиля и многословности модели
  - Параллельные запросы экономят время в ~4 раза
- Формат сдачи: код + демонстрация (видео с запуском `/compare` на разных промптах).
- Префикс коммита: `feat: AI Advent Challenge 7 – model comparison`

## Задание 8
- Формулировка: добавить в код подсчёт токенов для запроса и ответа и сравнить поведение модели на коротком, длинном и очень длинном запросах (приближающихся к лимиту контекста).
- Реализация: команда `/tokens` в Chainlit, которая запускает три запроса к самой дешёвой (бесплатной) модели из задания 7 и собирает метрики токенов из поля `usage` ответа API.
- Результат:
  - Для каждого сценария (короткий, длинный, очень длинный запрос) показываются:
    - `prompt_tokens` (входные токены),
    - `completion_tokens` (выходные токены),
    - `total_tokens`,
    - полный текст запроса и полный ответ модели (или ошибка API).
  - Видно, как при росте длины запроса:
    - растут входные токены,
    - ответы становятся более сжатыми,
    - при слишком длинном запросе ответ может обрезаться или возникнуть ошибка превышения лимита.
- Технические детали:
  - Новый метод `OpenRouterClient.token_length_experiment()` для запуска трёх сценариев на фиксированной бесплатной модели `nvidia/nemotron-nano-12b-v2-vl:free`.
  - Используются реальные значения `prompt_tokens`, `completion_tokens`, `total_tokens` из `api_response["usage"]` без собственных эвристик подсчёта токенов.
  - Функция `format_token_experiment_results()` форматирует результаты в Markdown-таблицу и детальное описание для команды `/tokens`.
  - Команда `/tokens` интегрирована через обработчик `handle_tokens_command()` и проверку в `@cl.on_message`.
- Формат сдачи: код + демонстрация (видео с запуском `/tokens` и разбором результатов трёх сценариев).
- Префикс коммита: `feat: AI Advent Challenge 8 – token usage experiment`
