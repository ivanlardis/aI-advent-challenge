# Спецификация: Локальный аналитик данных (Day 29)

## Общее описание

Локальное приложение для анализа структурированных данных (CSV, JSON, логи) с использованием LLM-модели через Ollama. Весь анализ происходит локально без отправки данных в облако.

---

## Техническая реализация

### Архитектура

- **Тип**: Монолитное приложение в одном файле (`app.py`)
- **Фреймворк UI**: Chainlit
- **LLM**: Ollama с моделью `qwen2.5-coder:7b`
- **Язык**: Python 3.9+
- **Структура проекта**: ai-advent/day28/ (перезапись существующего проекта)

### Зависимости (минимальный набор)

```txt
chainlit>=1.0.0
requests>=2.31.0
```

**Обоснование**: Используем только стандартные библиотеки Python + Chainlit + requests для HTTP-запросов к Ollama API. Парсинг CSV/JSON через встроенные `csv` и `json` модули.

### Ключевые компоненты

1. **Парсер данных** (автоопределение формата)
   - Определение типа файла по расширению (.csv, .json, .log)
   - Валидация и обработка ошибок (пропуск невалидных строк)
   - Конвертация всех форматов в структурированный JSON для LLM

2. **Интеграция с Ollama**
   - Проверка доступности Ollama при старте (health check)
   - Попытка автоматического запуска `ollama serve` при недоступности
   - Работа с Ollama REST API через requests

3. **Chainlit UI**
   - Загрузка файлов через встроенный file upload
   - Чат-интерфейс для вопросов
   - Адаптивные примеры вопросов в зависимости от типа загруженного файла

4. **Промпт-инжиниринг**
   - Структурированный формат: System prompt + данные (JSON) + инструкция + вопрос пользователя
   - Контекстная стратегия: смешанная (история диалога + данные только в первом запросе)

---

## UI/UX

### Интерфейс пользователя

- **Главный экран**: Chainlit чат с возможностью загрузки файла
- **Загрузка данных**: Через UI file upload (максимум 10MB)
- **Вывод результатов**:
  - Текстовые ответы
  - Статистика и метрики в структурированном виде
- **Примеры вопросов**: Адаптивные подсказки после загрузки файла

### Адаптивные примеры вопросов

**Для CSV**:
- "Сколько всего записей в файле?"
- "Какие уникальные значения в колонке X?"
- "Какая средняя/максимальная/минимальная по колонке Y?"

**Для JSON**:
- "Какая структура данных?"
- "Сколько объектов на верхнем уровне?"
- "Какие поля присутствуют во всех записях?"

**Для LOG**:
- "Какая ошибка встречается чаще всего?"
- "Сколько записей уровня ERROR/WARN/INFO?"
- "В какое время произошло больше всего ошибок?"

### Обработка пользовательского ввода

- **Off-topic вопросы**: Вежливое перенаправление к анализу данных ("Я специализируюсь на анализе данных. Загрузите файл и задайте вопрос о его содержимом.")
- **Непонятные вопросы**: Честное признание ("Не могу понять ваш вопрос. Попробуйте переформулировать или спросите что-то конкретное о данных.")

---

## Обработка данных

### Форматы и парсинг

1. **CSV**
   - Автоопределение разделителя (`,` или `;`)
   - Чтение заголовков
   - Конвертация в список словарей
   - Обработка ошибок: пропуск строк с неверным количеством колонок

2. **JSON**
   - Валидация JSON-структуры
   - Поддержка массивов и вложенных объектов
   - Обработка ошибок: сообщение о невалидном JSON

3. **LOG (текстовые логи)**
   - Построчное чтение
   - Передача в LLM "as is" (без специальной группировки)
   - Пропуск пустых строк

### Ограничения

- **Максимальный размер файла**: 10MB (жёсткий лимит)
- **Пустые файлы**: Сообщение об ошибке с просьбой загрузить другой файл
- **Невалидные данные**: Пропуск с продолжением обработки

---

## LLM и промпты

### Конфигурация Ollama

- **Модель**: `qwen2.5-coder:7b` (хардкод в коде)
- **URL**: `http://localhost:11434` (хардкод в коде)
- **Проверка доступности**:
  1. Health check при старте приложения
  2. Попытка автоматического запуска через `subprocess.Popen(['ollama', 'serve'])`
  3. Повторная проверка через 3 секунды
  4. Если всё ещё недоступно — вывод инструкции пользователю

### Структура промпта

```
SYSTEM:
Ты — аналитик данных. Отвечай на вопросы пользователя на основе предоставленных данных.
Будь точным и конкретным. Если не можешь ответить на вопрос — честно скажи об этом.

DATA:
{структурированные данные в JSON-формате}

INSTRUCTION:
Проанализируй данные выше и ответь на следующий вопрос пользователя.

USER QUESTION:
{вопрос пользователя}
```

### Передача данных модели

- **Формат**: Структурированный JSON
- **Полнота**: Весь файл (<10MB) передаётся целиком
- **Контекст**: Данные передаются только в первом сообщении сессии, далее модель работает на основе истории диалога

### Обработка ответов

- **Без кэширования**: Каждый запрос — новый вызов LLM
- **Без streaming**: Простое ожидание полного ответа
- **Многоязычность**: Модель сама определяет и отвечает на языке вопроса (русский/английский)

---

## Обработка ошибок и граничные случаи

### Сценарии ошибок

| Ситуация | Поведение |
|----------|-----------|
| Ollama не запущена | Попытка автозапуска → инструкция пользователю |
| Файл >10MB | Отказ в загрузке, сообщение о лимите |
| Пустой файл | Сообщение об ошибке |
| Невалидные строки в CSV | Пропуск, продолжение обработки |
| Невалидный JSON | Сообщение об ошибке |
| Модель не понимает вопрос | "Не могу понять вопрос, переформулируйте" |
| Многострочные stack traces в логах | Передача в LLM as is |

### Безопасность

- **Чувствительные данные**: Не обрабатываются специально (всё локально, данные не покидают машину)
- **Disclaimer**: Отсутствует (подразумевается, что пользователь понимает локальность обработки)

---

## Тестирование

### Подход

- **Ручное тестирование** с набором примеров данных

### Тестовые данные

Создать в `ai-advent/day28/test_data/`:

1. **sample.csv**:
   ```csv
   id,name,age,city
   1,Alice,30,Moscow
   2,Bob,25,SPB
   3,Charlie,35,Moscow
   ```

2. **sample.json**:
   ```json
   [
     {"user_id": 1, "action": "login", "timestamp": "2024-01-20T10:00:00"},
     {"user_id": 2, "action": "purchase", "timestamp": "2024-01-20T10:05:00"},
     {"user_id": 1, "action": "logout", "timestamp": "2024-01-20T10:30:00"}
   ]
   ```

3. **sample.log**:
   ```
   2024-01-20 10:00:00 INFO User login: user_id=1
   2024-01-20 10:05:00 ERROR Failed to connect to DB
   2024-01-20 10:05:01 ERROR Failed to connect to DB
   2024-01-20 10:10:00 WARN High memory usage: 85%
   ```

### Сценарии тестирования

1. Загрузить CSV → спросить "Сколько пользователей из Москвы?"
2. Загрузить JSON → спросить "Какие действия выполнял user_id=1?"
3. Загрузить LOG → спросить "Какая ошибка чаще всего?"
4. Попробовать файл >10MB → убедиться в отказе
5. Попробовать пустой файл → увидеть сообщение об ошибке
6. Спросить off-topic вопрос → увидеть перенаправление

---

## Документация

### README.md

```markdown
# Локальный аналитик данных

Анализ CSV, JSON и логов с помощью локальной LLM-модели через Ollama.

## Требования

- Python 3.9+
- Ollama (установлен и запущен)

## Установка

```bash
pip install chainlit requests
```

## Запуск

```bash
chainlit run app.py
```

Приложение откроется в браузере на `http://localhost:8000`.

## Использование

1. Загрузите файл (CSV, JSON или LOG) через интерфейс
2. Задавайте вопросы о данных в чате
3. Получайте ответы на основе анализа LLM

## Примеры вопросов

**CSV**: "Сколько записей?", "Какие уникальные значения в колонке X?"
**JSON**: "Какая структура?", "Сколько объектов?"
**LOG**: "Какая ошибка чаще всего?", "Сколько ERROR-записей?"

## Ограничения

- Максимальный размер файла: 10MB
- Работает только локально (данные не отправляются в облако)
```

---

## Компромиссы и ограничения

### Принятые решения

1. **Всё в одном файле** → Простота развёртывания vs расширяемость
2. **Хардкод конфигурации** → Быстрое MVP vs гибкость
3. **Без кэширования** → Простота vs производительность
4. **Без streaming** → Меньше кода vs UX при медленных ответах
5. **LLM для всех агрегаций** → Простота vs точность (могут быть ошибки в подсчётах)
6. **Пропуск невалидных данных** → Устойчивость vs строгость
7. **10MB лимит** → Защита от перегрузки vs гибкость

### Риски

- **Точность ответов**: LLM может ошибаться в числовых подсчётах
- **Производительность**: Отправка 10MB данных в каждый запрос может быть медленной
- **Зависимость от Ollama**: Если Ollama упадёт во время сессии, приложение перестанет работать

### Пути улучшения (не для MVP)

- Использование pandas для точных агрегаций
- Chunking для больших файлов
- Кэширование распарсенных данных
- Streaming ответов
- .env конфигурация
- Модульная структура

---

## План реализации

1. ✅ Создать спецификацию (SPEC.md)
2. Создать структуру проекта в ai-advent/day28/
3. Написать app.py (весь функционал в одном файле)
4. Создать test_data/ с примерами
5. Написать README.md
6. Создать requirements.txt
7. Протестировать все сценарии
8. Коммит изменений

---

## Критерии успеха

- ✅ Приложение запускается через `chainlit run app.py`
- ✅ Можно загрузить CSV, JSON, LOG файлы через UI
- ✅ LLM корректно отвечает на аналитические вопросы
- ✅ Ошибки обрабатываются gracefully
- ✅ Весь анализ происходит локально
- ✅ Есть README с инструкцией по запуску

---

## Дата создания спецификации

2026-01-23
