#!/usr/bin/env python3
"""
–õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö
–ê–Ω–∞–ª–∏–∑ CSV, JSON –∏ –ª–æ–≥–æ–≤ —Å –ø–æ–º–æ—â—å—é –ª–æ–∫–∞–ª—å–Ω–æ–π LLM —á–µ—Ä–µ–∑ Ollama
"""

import asyncio
import csv
import json
import os
import subprocess
import time
from pathlib import Path
from typing import Dict, List, Any, Optional

import chainlit as cl
import requests

# Chunked-–æ–±—Ä–∞–±–æ—Ç–∫–∞
import chunking


# ========================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ==========================

OLLAMA_URL = "http://localhost:11434"
OLLAMA_MODEL = "qwen2.5:0.5b"  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –Ω–∞ qwen
MAX_FILE_SIZE_MB = 10
MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024


# ========================== OLLAMA UTILS ==========================

def check_ollama_health() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama"""
    try:
        response = requests.get(f"{OLLAMA_URL}/api/tags", timeout=5)
        return response.status_code == 200
    except Exception:
        return False


def try_start_ollama() -> bool:
    """–ü—ã—Ç–∞–µ—Ç—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å Ollama –≤ —Ñ–æ–Ω–µ"""
    try:
        subprocess.Popen(
            ['ollama', 'serve'],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL
        )
        time.sleep(3)  # –î–∞—ë–º –≤—Ä–µ–º—è –Ω–∞ –∑–∞–ø—É—Å–∫
        return check_ollama_health()
    except Exception:
        return False


def ensure_ollama_running() -> str:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω–∞, –ø—ã—Ç–∞–µ—Ç—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å –µ—Å–ª–∏ –Ω–µ—Ç.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –æ —Å—Ç–∞—Ç—É—Å–µ.
    """
    if check_ollama_health():
        return "‚úÖ Ollama –¥–æ—Å—Ç—É–ø–Ω–∞"

    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å
    if try_start_ollama():
        return "‚úÖ Ollama –∑–∞–ø—É—â–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏"

    # –ù–µ —É–¥–∞–ª–æ—Å—å
    return """
‚ùå Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞!

–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–ø—É—Å—Ç–∏—Ç–µ Ollama –≤—Ä—É—á–Ω—É—é:
```bash
ollama serve
```

–ó–∞—Ç–µ–º –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É.
"""


def call_ollama(prompt: str) -> str:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ Ollama –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç"""
    try:
        response = requests.post(
            f"{OLLAMA_URL}/api/generate",
            json={
                "model": OLLAMA_MODEL,
                "prompt": prompt,
                "stream": False
            },
            timeout=60
        )

        if response.status_code == 200:
            result = response.json()
            return result.get("response", "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç")
        else:
            return f"–û—à–∏–±–∫–∞ Ollama API: {response.status_code}"

    except requests.exceptions.Timeout:
        return "‚è±Ô∏è –ó–∞–ø—Ä–æ—Å –ø—Ä–µ–≤—ã—Å–∏–ª –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è (60 —Å–µ–∫)"
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Ollama: {str(e)}"


# ========================== CHUNKED-–û–ë–†–ê–ë–û–¢–ö–ê ==========================

async def process_chunk(chunk: Dict, question: str, chunk_num: int, total_chunks: int) -> Dict:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —á–∞–Ω–∫ —á–µ—Ä–µ–∑ LLM —Å retry –ª–æ–≥–∏–∫–æ–π.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.
    """
    max_retries = 2
    prompt = chunking.build_chunk_prompt(chunk, question, chunk_num, total_chunks)

    # –û—Ç–ª–∞–¥–∫–∞
    print(f"\nüîç –ß–∞–Ω–∫ {chunk_num+1}/{total_chunks}:")
    print(f"üì¶ –í–æ–ø—Ä–æ—Å: {question}")
    print(f"üìä –î–∞–Ω–Ω—ã—Ö –≤ —á–∞–Ω–∫–µ: {len(chunk.get('data', []))} —Å—Ç—Ä–æ–∫/—ç–ª–µ–º–µ–Ω—Ç–æ–≤")
    print(f"üìù –î–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤")

    for attempt in range(max_retries):
        print(f"  ‚Üí –ü–æ–ø—ã—Ç–∫–∞ {attempt+1}/{max_retries}")
        response = call_ollama(prompt)
        print(f"  ‚Üê –û—Ç–≤–µ—Ç: {response[:100]}...")  # –ü–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤

        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
        try:
            # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—ë—Ä–Ω—É—Ç –≤ —Ç–µ–∫—Å—Ç)
            response = response.strip()

            # –ï—Å–ª–∏ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å ```json, –∏–∑–≤–ª–µ–∫–∞–µ–º
            if response.startswith("```json"):
                response = response[7:]
            if response.startswith("```"):
                response = response[3:]
            if response.endswith("```"):
                response = response[:-3]

            response = response.strip()

            result = json.loads(response)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
            if "count" not in result:
                result["count"] = 0
            if "items" not in result:
                result["items"] = []
            if "summary" not in result:
                result["summary"] = ""

            return result

        except json.JSONDecodeError:
            if attempt < max_retries - 1:
                # Retry —Å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–º –ø—Ä–æ–º–ø—Ç–æ–º
                prompt = chunking.make_stricter_prompt(prompt)
            else:
                # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞: –∏–∑–≤–ª–µ—á—å —á–∏—Å–ª–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞
                count = chunking.extract_number_from_text(response)
                return {
                    "count": count,
                    "items": [],
                    "summary": response[:200]  # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
                }

    # Fallback (–Ω–µ –¥–æ–ª–∂–Ω–æ –¥–æ–π—Ç–∏ —Å—é–¥–∞)
    return {"count": 0, "items": [], "summary": "–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–∞"}


async def process_chunked(data: Dict, question: str) -> str:
    """
    –ü–æ–ª–Ω–∞—è chunked-–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.
    """
    import logging
    logger = logging.getLogger(__name__)

    # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –∏ —Å–æ–∑–¥–∞—ë–º —á–∞–Ω–∫–∏
    total_count = chunking.get_total_count(data)
    chunk_size = chunking.calculate_chunk_size(total_count)
    chunks = chunking.chunk_data(data, chunk_size)

    # –ò–Ω—Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ chunked-—Ä–µ–∂–∏–º–µ
    info_msg = f"üìä –§–∞–π–ª –±–æ–ª—å—à–æ–π ({total_count} —Å—Ç—Ä–æ–∫). –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ —á–∞—Å—Ç—è–º: {len(chunks)} —á–∞–Ω–∫–æ–≤ –ø–æ {chunk_size} —Å—Ç—Ä–æ–∫."
    await cl.Message(content=info_msg).send()

    # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
    progress_msg = await cl.Message(
        content=f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ 0/{len(chunks)} (0%)"
    ).send()

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫–∏
    chunk_results = []
    import time as time_module
    start_time = time_module.time()

    for i, chunk in enumerate(chunks):
        chunk_start = time_module.time()
        result = await process_chunk(chunk, question, i, len(chunks))
        chunk_elapsed = time_module.time() - chunk_start

        chunk_results.append(result)

        # –õ–æ–≥–∏—Ä—É–µ–º –≤—Ä–µ–º—è
        logger.info(f"Chunk {i+1}/{len(chunks)} processed in {chunk_elapsed:.2f}s")
        print(f"‚è±Ô∏è –ß–∞–Ω–∫ {i+1}/{len(chunks)}: {chunk_elapsed:.2f} —Å–µ–∫")

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        percent = int((i + 1) / len(chunks) * 100)
        progress_msg.content = f"üîÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(chunks)} ({percent}%) | –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫: {chunk_elapsed:.1f}s"
        await progress_msg.update()

    total_elapsed = time_module.time() - start_time
    logger.info(f"Total processing time: {total_elapsed:.2f}s")
    print(f"‚è±Ô∏è –í–°–ï–ì–û: {total_elapsed:.2f} —Å–µ–∫")

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è
    progress_msg.content = "‚ú® –§–æ—Ä–º–∏—Ä—É—é –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç..."
    await progress_msg.update()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–∂–Ω–æ –ª–∏ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å –≤ –∫–æ–¥–µ (–ø—Ä–æ—Å—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏)
    if is_simple_aggregation(question):
        # –ü—Ä–æ—Å—Ç–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è: —Å—É–º–º–∞ –≤ –∫–æ–¥–µ
        aggregated = chunking.aggregate_simple(chunk_results)
        total = aggregated["total_count"]
        items = aggregated["all_items"]

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LLM
        final_prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ {len(chunks)} —á–∞—Å—Ç–µ–π –¥–∞–Ω–Ω—ã—Ö:

–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {total}
{f'–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(items)}' if items else ''}

–í–æ–ø—Ä–æ—Å –±—ã–ª: {question}

–î–∞–π –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
"""
        answer = call_ollama(final_prompt)

    else:
        # –°–ª–æ–∂–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è: —á–µ—Ä–µ–∑ LLM
        aggregation_prompt = chunking.build_aggregation_prompt(chunk_results, question)
        answer = call_ollama(aggregation_prompt)

    await progress_msg.remove()
    return answer


def is_simple_aggregation(question: str) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—É—é –∞–≥—Ä–µ–≥–∞—Ü–∏—é (—Å—É–º–º–∞ –≤ –∫–æ–¥–µ)"""
    simple_keywords = ["—Å–∫–æ–ª—å–∫–æ", "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ", "count", "—á–∏—Å–ª–æ", "–≤—Å–µ–≥–æ"]
    question_lower = question.lower()
    return any(keyword in question_lower for keyword in simple_keywords)


# ========================== –ü–ê–†–°–ï–†–´ –î–ê–ù–ù–´–• ==========================

def parse_csv(file_path: str) -> Dict[str, Any]:
    """
    –ü–∞—Ä—Å–∏—Ç CSV —Ñ–∞–π–ª –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON.
    –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å, –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏.
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è
            sample = f.read(1024)
            f.seek(0)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
            sniffer = csv.Sniffer()
            delimiter = sniffer.sniff(sample).delimiter

            # –ß–∏—Ç–∞–µ–º CSV
            reader = csv.DictReader(f, delimiter=delimiter)
            rows = []
            skipped = 0

            for i, row in enumerate(reader):
                try:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—É—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –¥–ª—è –≤—Å–µ—Ö –ø–æ–ª–µ–π
                    if all(v.strip() == '' for v in row.values()):
                        skipped += 1
                        continue
                    rows.append(row)
                except Exception:
                    skipped += 1

            return {
                "format": "csv",
                "columns": list(rows[0].keys()) if rows else [],
                "row_count": len(rows),
                "skipped_rows": skipped,
                "data": rows
            }

    except Exception as e:
        raise ValueError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ CSV: {str(e)}")


def parse_json(file_path: str) -> Dict[str, Any]:
    """–ü–∞—Ä—Å–∏—Ç JSON —Ñ–∞–π–ª"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø JSON (–º–∞—Å—Å–∏–≤ –∏–ª–∏ –æ–±—ä–µ–∫—Ç)
            if isinstance(data, list):
                return {
                    "format": "json",
                    "type": "array",
                    "count": len(data),
                    "data": data
                }
            elif isinstance(data, dict):
                return {
                    "format": "json",
                    "type": "object",
                    "keys": list(data.keys()),
                    "data": data
                }
            else:
                return {
                    "format": "json",
                    "type": "primitive",
                    "data": data
                }

    except json.JSONDecodeError as e:
        raise ValueError(f"–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON: {str(e)}")
    except Exception as e:
        raise ValueError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ JSON: {str(e)}")


def parse_log(file_path: str) -> Dict[str, Any]:
    """–ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ª–æ–≥-—Ñ–∞–π–ª (–±–µ–∑ —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è - chunking –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å—ë)"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = []

            for line in f:
                line = line.strip()
                if line:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                    lines.append(line)

            return {
                "format": "log",
                "line_count": len(lines),
                "data": lines
            }

    except Exception as e:
        raise ValueError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ LOG: {str(e)}")


def parse_file(file_path: str) -> Dict[str, Any]:
    """
    –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –∏ –ø–∞—Ä—Å–∏—Ç –µ–≥–æ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON.
    """
    extension = Path(file_path).suffix.lower()

    if extension == '.csv':
        return parse_csv(file_path)
    elif extension == '.json':
        return parse_json(file_path)
    elif extension in ['.log', '.txt']:
        return parse_log(file_path)
    else:
        raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {extension}")


# ========================== –ü–†–û–ú–ü–¢ –ò–ù–ñ–ò–ù–ò–†–ò–ù–ì ==========================

def build_analysis_prompt(data: Dict[str, Any], question: str) -> str:
    """–°–æ–∑–¥–∞—ë—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM"""

    fmt = data.get("format", "")

    # –£–ø—Ä–æ—â–∞–µ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–π –º–æ–¥–µ–ª–∏
    if fmt == "csv":
        # –î–ª—è CSV - –ø—Ä–æ—Å—Ç–æ–µ —Ç–∞–±–ª–∏—á–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
        data_text = "–ö–æ–ª–æ–Ω–∫–∏: " + ", ".join(data["columns"]) + "\n\n"
        data_text += "–î–∞–Ω–Ω—ã–µ:\n"
        for row in data["data"]:
            data_text += str(row) + "\n"

    elif fmt == "json":
        data_type = data.get("type", "")
        if data_type == "array":
            data_text = f"–ú–∞—Å—Å–∏–≤ –∏–∑ {data['count']} —ç–ª–µ–º–µ–Ω—Ç–æ–≤:\n"
            for i, item in enumerate(data["data"][:50]):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                data_text += f"{i+1}. {json.dumps(item, ensure_ascii=False)}\n"
        else:
            data_text = json.dumps(data["data"], ensure_ascii=False, indent=2)

    elif fmt == "log":
        data_text = f"–õ–æ–≥-—Ñ–∞–π–ª ({data['line_count']} —Å—Ç—Ä–æ–∫):\n\n"

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥ –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ (–±–µ–∑ chunking)
        for line in data["data"][:200]:  # –ü–µ—Ä–≤—ã–µ 200 —Å—Ç—Ä–æ–∫ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            data_text += line + "\n"
    else:
        data_text = json.dumps(data, ensure_ascii=False, indent=2)

    system_prompt = ""

    data_section = f"–õ–û–ì:\n{data_text}"

    instruction = f"""–í–û–ü–†–û–°: {question}

–û–¢–í–ï–¢–¨ –ü–û-–†–£–°–°–ö–ò –ö–†–ê–¢–ö–û:"""

    question_section = ""

    return f"{system_prompt}\n\n{data_section}\n\n{instruction}\n\n{question_section}"


# ========================== –ê–î–ê–ü–¢–ò–í–ù–´–ï –ü–†–ò–ú–ï–†–´ ==========================

def get_suggested_questions(data_format: str) -> List[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö"""

    suggestions = {
        "csv": [
            "–°–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ —Ñ–∞–π–ª–µ?",
            "–ö–∞–∫–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–∞–∂–¥–æ–π –∫–æ–ª–æ–Ω–∫–µ?",
            "–ü–æ–∫–∞–∂–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø–æ–ª—è–º",
            "–ï—Å—Ç—å –ª–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã?"
        ],
        "json": [
            "–ö–∞–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö?",
            "–°–∫–æ–ª—å–∫–æ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –≤–µ—Ä—Ö–Ω–µ–º —É—Ä–æ–≤–Ω–µ?",
            "–ö–∞–∫–∏–µ –ø–æ–ª—è –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤–æ –≤—Å–µ—Ö –∑–∞–ø–∏—Å—è—Ö?",
            "–ü–æ–∫–∞–∂–∏ —Å–≤–æ–¥–∫—É –ø–æ –¥–∞–Ω–Ω—ã–º"
        ],
        "log": [
            "–ö–∞–∫–∞—è –æ—à–∏–±–∫–∞ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —á–∞—â–µ –≤—Å–µ–≥–æ?",
            "–°–∫–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–µ–π —É—Ä–æ–≤–Ω—è ERROR/WARN/INFO?",
            "–í –∫–∞–∫–æ–µ –≤—Ä–µ–º—è –ø—Ä–æ–∏–∑–æ—à–ª–æ –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ –æ—à–∏–±–æ–∫?",
            "–ö–∞–∫–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –≤ –ª–æ–≥–µ?"
        ]
    }

    return suggestions.get(data_format, [])


# ========================== CHAINLIT HANDLERS ==========================

@cl.on_chat_start
async def start():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–∞—Ç–∞"""

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Ollama
    status = ensure_ollama_running()

    welcome_msg = f"""# üîç –õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö

{status}

–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª (CSV, JSON –∏–ª–∏ LOG) –¥–ª—è –Ω–∞—á–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∞.

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
- üìä –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è chunked-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è >1000 —Å—Ç—Ä–æ–∫)
- üîÑ –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
- üéØ –¢–æ—á–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ –≤—Å–µ—Ö —á–∞—Å—Ç–µ–π —Ñ–∞–π–ª–∞

**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {MAX_FILE_SIZE_MB}MB
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: .csv, .json, .log, .txt
- –ú–∞–∫—Å–∏–º—É–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö —á–∞–Ω–∫–æ–≤: 100
"""

    await cl.Message(content=welcome_msg).send()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Å—Å–∏—é
    cl.user_session.set("data", None)
    cl.user_session.set("data_format", None)


@cl.on_message
async def main(message: cl.Message):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""

    # –û—Ç–ª–∞–¥–∫–∞
    print(f"\n{'='*60}")
    print(f"üîî –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ:")
    print(f"  Content: {message.content[:100] if message.content else 'None'}")
    print(f"  Elements: {len(message.elements) if message.elements else 0}")
    print(f"{'='*60}\n")

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    if message.elements:
        print("üìÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞...")
        await handle_file_upload(message.elements)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞ –≤–º–µ—Å—Ç–µ —Å —Ñ–∞–π–ª–æ–º
        question = message.content.strip() if message.content else ""
        if question:
            print(f"‚ùì –ï—Å—Ç—å –≤–æ–ø—Ä–æ—Å –≤–º–µ—Å—Ç–µ —Å —Ñ–∞–π–ª–æ–º: {question}")
            # –î–∞—ë–º —Ñ–∞–π–ª—É –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å—Å—è –∏ –∂–¥—ë–º –Ω–µ–º–Ω–æ–≥–æ
            await asyncio.sleep(1)

            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            data = cl.user_session.get("data")
            if data:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å
                if chunking.should_use_chunking(data):
                    print("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è chunked-–æ–±—Ä–∞–±–æ—Ç–∫–∞")
                    answer = await process_chunked(data, question)
                    await cl.Message(content=answer).send()
                else:
                    print("üìù –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞")
                    prompt = build_analysis_prompt(data, question)
                    thinking_msg = await cl.Message(content="ü§î –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –¥–∞–Ω–Ω—ã–µ...").send()
                    answer = call_ollama(prompt)
                    await thinking_msg.remove()
                    await cl.Message(content=answer).send()

        return

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ª–∏ –¥–∞–Ω–Ω—ã–µ
    data = cl.user_session.get("data")

    print(f"üìä –î–∞–Ω–Ω—ã–µ –≤ —Å–µ—Å—Å–∏–∏: {data is not None}")
    if data:
        print(f"  –§–æ—Ä–º–∞—Ç: {data.get('format')}")
        print(f"  –°—Ç—Ä–æ–∫: {data.get('line_count', data.get('row_count', data.get('count', 0)))}")

    if data is None:
        await cl.Message(
            content="‚ùå –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."
        ).send()
        return

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    question = message.content.strip()
    print(f"‚ùì –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}")

    if not question:
        await cl.Message(content="–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –æ –¥–∞–Ω–Ω—ã—Ö.").send()
        return

    # –í—ã–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏
    if chunking.should_use_chunking(data):
        print("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è chunked-–æ–±—Ä–∞–±–æ—Ç–∫–∞")
        # Chunked-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
        answer = await process_chunked(data, question)
        await cl.Message(content=answer).send()

    else:
        print("üìù –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞")
        # –û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
        prompt = build_analysis_prompt(data, question)
        print(f"üì§ –î–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤")

        thinking_msg = await cl.Message(content="ü§î –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –¥–∞–Ω–Ω—ã–µ...").send()
        answer = call_ollama(prompt)

        await thinking_msg.remove()
        await cl.Message(content=answer).send()


async def handle_file_upload(elements: List):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""

    file_element = elements[0]  # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π —Ñ–∞–π–ª

    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ (–Ω–æ–≤—ã–π —Ñ–∞–π–ª = –Ω–æ–≤–∞—è —Å–µ—Å—Å–∏—è)
    previous_data = cl.user_session.get("data")
    if previous_data is not None:
        await cl.Message(
            content="üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω –Ω–æ–≤—ã–π —Ñ–∞–π–ª. –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ —Å–±—Ä–æ—à–µ–Ω–∞."
        ).send()

    # –û—á–∏—â–∞–µ–º —Å–µ—Å—Å–∏—é
    cl.user_session.set("data", None)
    cl.user_session.set("data_format", None)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
    file_size = os.path.getsize(file_element.path)
    if file_size > MAX_FILE_SIZE_BYTES:
        size_mb = file_size / (1024 * 1024)
        await cl.Message(
            content=f"‚ùå –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({size_mb:.1f}MB). –ú–∞–∫—Å–∏–º—É–º: {MAX_FILE_SIZE_MB}MB"
        ).send()
        return

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –Ω–µ –ø—É—Å—Ç–æ–π
    if file_size == 0:
        await cl.Message(
            content="‚ùå –§–∞–π–ª –ø—É—Å—Ç–æ–π. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏."
        ).send()
        return

    # –ü–∞—Ä—Å–∏–º —Ñ–∞–π–ª
    try:
        data = parse_file(file_element.path)
        data_format = data["format"]

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Å–µ—Å—Å–∏—é
        cl.user_session.set("data", data)
        cl.user_session.set("data_format", data_format)

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–≥—Ä—É–∑–∫–µ
        if data_format == "csv":
            info = f"**CSV —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω!**\n\n"
            info += f"- –ö–æ–ª–æ–Ω–æ–∫: {len(data['columns'])}\n"
            info += f"- –°—Ç—Ä–æ–∫: {data['row_count']}\n"
            if data['skipped_rows'] > 0:
                info += f"- –ü—Ä–æ–ø—É—â–µ–Ω–æ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {data['skipped_rows']}\n"

        elif data_format == "json":
            info = f"**JSON —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω!**\n\n"
            if data['type'] == 'array':
                info += f"- –¢–∏–ø: –º–∞—Å—Å–∏–≤\n"
                info += f"- –≠–ª–µ–º–µ–Ω—Ç–æ–≤: {data['count']}\n"
            elif data['type'] == 'object':
                info += f"- –¢–∏–ø: –æ–±—ä–µ–∫—Ç\n"
                info += f"- –ö–ª—é—á–µ–π: {len(data['keys'])}\n"

        elif data_format == "log":
            info = f"**LOG —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω!**\n\n"
            info += f"- –°—Ç—Ä–æ–∫: {data['line_count']}\n"

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ chunked-–æ–±—Ä–∞–±–æ—Ç–∫–µ
        if chunking.should_use_chunking(data):
            total_count = chunking.get_total_count(data)
            chunk_size = chunking.calculate_chunk_size(total_count)
            num_chunks = (total_count + chunk_size - 1) // chunk_size  # –û–∫—Ä—É–≥–ª–µ–Ω–∏–µ –≤–≤–µ—Ä—Ö

            info += f"\nüí° **–†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏:** Chunked (—Ñ–∞–π–ª –±–æ–ª—å—à–æ–π)\n"
            info += f"- –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–∞—Å—Ç—è–º–∏: ~{num_chunks} —á–∞–Ω–∫–æ–≤ –ø–æ {chunk_size} —Å—Ç—Ä–æ–∫\n"
        else:
            info += f"\nüí° **–†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏:** –û–±—ã—á–Ω—ã–π (–æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º)\n"

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤
        suggestions = get_suggested_questions(data_format)
        if suggestions:
            info += f"\n**–ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤:**\n"
            for q in suggestions:
                info += f"- {q}\n"

        await cl.Message(content=info).send()

    except ValueError as e:
        await cl.Message(
            content=f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞:\n{str(e)}"
        ).send()
    except Exception as e:
        await cl.Message(
            content=f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞:\n{str(e)}"
        ).send()


# ========================== ENTRY POINT ==========================

if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Ollama –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ Ollama...")
    status = ensure_ollama_running()
    print(status)

    if "‚ùå" not in status:
        print(f"\n‚úÖ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ –∫ –∑–∞–ø—É—Å–∫—É!")
        print(f"üìä –ú–æ–¥–µ–ª—å: {OLLAMA_MODEL}")
        print(f"üåê Ollama URL: {OLLAMA_URL}")
