# Спецификация: local-llm-cli

## Назначение
Простой CLI-клиент на Kotlin для взаимодействия с локальной LLM через Ollama API.

## Архитектурные решения

### Формат интерфейса
- **CLI (терминал)** — минимум зависимостей, фокус на API

### Хранение состояния
- **Только в RAM** — текущая сессия живёт в памяти. Перезапуск = всё забыто

### Выбор модели
- **Динамический список** — парсим `/api/tags` и показываем доступные модели

### Безопасность
- **Без защиты** — Ollama по умолчанию работает без авторизации на localhost

### Потоковая передача
- **Ждать полностью** — `"stream": false` в API, выводим ответ целиком

### Обработка ошибок
- **Просто exit** — при ошибке крашимся с traceback

### Конфигурация
- **Хардкод** — `localhost:11434` захардкожен

### Стек технологий
- **Язык**: Kotlin
- **HTTP-клиент**: khttp
- **JSON**: Kotlinx.serialization
- **Сборка**: Gradle
- **Запуск**: JAR

### Структура проекта
- **Один файл** — вся логика в одном файле

### Режим работы
- **Однократный запрос** — один вопрос → ответ → exit

### Параметры генерации
- **Без параметров** — используем дефолты Ollama

### Вывод
- **Только текст** — выводим только ответ LLM

### Аргументы CLI
- **Позиционный** — `./script "текст prompt"`

### Выбор модели по умолчанию
- **Первая доступная** — если не указана, берём первую из `/api/tags`

### Асинхронность
- **Блокирующий** — `runBlocking { }`

### Название
- **local-llm-cli**

---

## API Ollama

### GET /api/tags
```json
{
  "models": [
    {"name": "gemma3:1b", ...},
    ...
  ]
}
```

### POST /api/generate
```json
{
  "model": "gemma3:1b",
  "prompt": "Вопрос",
  "stream": false
}
```

Ответ:
```json
{
  "model": "gemma3:1b",
  "response": "Ответ",
  "done": true
}
```

---

## CLI интерфейс

```
Usage: java -jar local-llm-cli.jar [prompt] [-m|--model <model>]

Аргументы:
  prompt        Текст запроса к LLM (позиционный)

Опции:
  -m, --model   Модель из списка (по умолчанию — первая доступная)
  -l, --list    Показать список доступных моделей и выйти
  -h, --help    Показать справку

Примеры:
  java -jar local-llm-cli.jar "Составь список из 5 идей для ужина."
  java -jar local-llm-cli.jar "Кто написал Войну и мир?" -m gemma3:1b
  java -jar local-llm-cli.jar -l
```

---

## Структура Gradle проекта

```
.
├── build.gradle.kts
├── settings.gradle.kts
└── src/main/kotlin/Main.kt
```

### Зависимости
```kotlin
dependencies {
    implementation("khttp:khttp:1.6.0")
    implementation("org.jetbrains.kotlinx:kotlinx-serialization-json:1.6.0")
}
```

---

## Требования
- Ollama запущена на `localhost:11434`
- Java/JDK 17+
